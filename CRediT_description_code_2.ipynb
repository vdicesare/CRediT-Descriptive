{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ac7dd-d678-43b5-9f12-0c42c3ff2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"proj_1060_labor_division\"\n",
    "permissions = \"fulldata\" # please use the readonly-v3 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3676a-f888-40ac-9354-da06221a7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Snippets/header_008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6551931-7b0d-4366-86d9-673a700acea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a Parquet file using Spark\n",
    "# Specify the file path as \"/Projects/proj_1060_labor_division/df_final.parquet\"\n",
    "df_final = spark.read.parquet(\"/Projects/proj_1060_labor_division/df_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6871f9-7b0d-4124-8a80-ef3535bb53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a description analysis dataset\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import split, explode, col, count, max, udf, min, collect_list\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Drop the \"source\" and \"issn\" columns from the DataFrame\n",
    "description_df = df_final.drop(\"source\").drop(\"issn\")\n",
    "\n",
    "# Split the \"credit\" column into individual elements and aggregate by \"doi\", \"year\", \"subjareas\", \"aunum\", \"auid\", \"surname\", \"position\"\n",
    "description_df = description_df.withColumn(\"credit\", explode(split(col(\"credit\"), \", \")))\n",
    "description_df = description_df.groupBy(\"doi\", \"year\", \"subjareas\", \"aunum\", \"auid\", \"surname\", \"position\").agg(collect_list(\"credit\").alias(\"credit\"))\n",
    "\n",
    "# Calculate the row count for each \"doi\"\n",
    "window_spec = Window.partitionBy(\"doi\")\n",
    "description_df = description_df.withColumn(\"row_count\", count(\"doi\").over(window_spec))\n",
    "\n",
    "# Calculate the maximum \"position\" for each \"doi\" and create a new column\n",
    "position_maxd = description_df.groupBy(\"doi\").agg(max(\"position\").alias(\"position_max\"))\n",
    "description_df = description_df.join(position_maxd, [\"doi\"], \"left_outer\")\n",
    "\n",
    "# Calculate the minimum \"position\" for each \"doi\" and create a new column\n",
    "position_mind = description_df.groupBy(\"doi\").agg(min(\"position\").alias(\"position_min\"))\n",
    "description_df = description_df.join(position_mind, [\"doi\"], \"left_outer\")\n",
    "\n",
    "\n",
    "# Define a UDF (User Defined Function) to determine \"discipline\" based on elements in \"subjareas\"\n",
    "def determine_discipline(subjareas):\n",
    "    health_sciences_keywords = [\"MEDI\", \"NURS\", \"VETE\", \"DENT\", \"HEAL\"]\n",
    "    life_sciences_keywords = [\"AGRI\", \"BIOC\", \"IMMU\", \"NEUR\", \"PHAR\"]\n",
    "    physical_sciences_keywords = [\"CENG\", \"CHEM\", \"COMP\", \"EART\", \"ENER\", \"ENGI\", \"ENVI\", \"MATE\", \"MATH\", \"PHYS\"]\n",
    "    social_sciences_keywords = [\"ARTS\", \"BUSI\", \"DECI\", \"ECON\", \"PSYC\", \"SOCI\"]\n",
    "    multidisciplinary_keywords = [\"MULT\"]\n",
    "\n",
    "    discipline = []\n",
    "    # Check for keywords in \"subjareas\" to determine the corresponding discipline\n",
    "    if any(keyword in subjareas for keyword in health_sciences_keywords):\n",
    "        discipline.append(\"Health Sciences\")\n",
    "    if any(keyword in subjareas for keyword in life_sciences_keywords):\n",
    "        discipline.append(\"Life Sciences\")\n",
    "    if any(keyword in subjareas for keyword in physical_sciences_keywords):\n",
    "        discipline.append(\"Physical Sciences\")\n",
    "    if any(keyword in subjareas for keyword in social_sciences_keywords):\n",
    "        discipline.append(\"Social Sciences\")\n",
    "    if any(keyword in subjareas for keyword in multidisciplinary_keywords):\n",
    "        discipline.append(\"Multidisciplinary\")\n",
    "\n",
    "    return discipline\n",
    "\n",
    "# Register the UDF with Spark\n",
    "determine_discipline_udf = udf(determine_discipline, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to add a new \"discipline\" column to the DataFrame\n",
    "description_df = description_df.withColumn(\"discipline\", determine_discipline_udf(col(\"subjareas\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21d8eb-69cf-4bba-8743-47f549e6b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data cleaning\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col, split, expr, regexp_replace, countDistinct, collect_set, explode, size\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Replace \"Formal Analysis\" with \"Formal analysis\" in the \"credit\" column\n",
    "description_df = description_df.withColumn(\"credit\", expr(\"transform(credit, x -> IF(x = 'Formal Analysis', 'Formal analysis', x))\"))\n",
    "\n",
    "# Remove elements not belonging to the CRediT14 categories\n",
    "# Define the list of allowed elements (remove elements not in CRediT14)\n",
    "allowed_elements = [\n",
    "    \"Conceptualization\", \"Data curation\", \"Formal analysis\", \"Funding acquisition\",\n",
    "    \"Investigation\", \"Methodology\", \"Project administration\", \"Resources\", \"Software\",\n",
    "    \"Supervision\", \"Validation\", \"Visualization\", \"Writing – original draft\", \"Writing – review & editing\"\n",
    "]\n",
    "\n",
    "# Define UDF to filter out disallowed elements\n",
    "def filter_elements(CRediT_list):\n",
    "    return [elem for elem in CRediT_list if elem in allowed_elements]\n",
    "# Register the UDF with Spark\n",
    "spark.udf.register(\"filter_elements\", filter_elements)\n",
    "# Apply the UDF to filter the \"credit\" column\n",
    "description_df = description_df.withColumn(\"credit\", expr(\"filter_elements(credit)\"))\n",
    "\n",
    "# Converts the credit column format from a string to a list for subsequent analysis\n",
    "# Remove rows where \"credit\" field is \"[]\"\n",
    "description_df = description_df.filter(col(\"credit\") != \"[]\")\n",
    "# Remove square brackets \"[\" and \"]\" from the \"credit\" column\n",
    "description_df = description_df.withColumn(\"credit\", regexp_replace(\"credit\", \"[\\\\[\\\\]]\", \"\"))\n",
    "# Parse the \"credit\" column into a list\n",
    "description_df = description_df.withColumn(\"credit\", split(col(\"credit\"), \", \"))\n",
    "\n",
    "# Define UDF to remove duplicate elements\n",
    "def remove_duplicates(arr):\n",
    "    return list(set(arr))\n",
    "remove_duplicates_udf = udf(remove_duplicates, ArrayType(StringType()))\n",
    "description_df = description_df.withColumn(\"credit\", remove_duplicates_udf(col(\"credit\")))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the number of elements in the \"credit\" list and add a new column to store the result. This represents the number of credits per author involved in a publication.\n",
    "description_df = description_df.withColumn(\"au_credit_num\", size(description_df[\"credit\"]))\n",
    "\n",
    "# Generate a new column \"paper_credit\" listing unique elements for each \"Eid\". This represents the number of credits that are all used in a publication.\n",
    "# Explode the \"credit\" column into separate rows\n",
    "credit_explode_df = description_df.withColumn(\"credit_exploded\", explode(description_df[\"credit\"])) \n",
    "# Aggregate to collect unique credits for each \"doi\"\n",
    "paper_credit = credit_explode_df.groupBy(\"year\", \"doi\").agg(collect_set(\"credit_exploded\").alias(\"paper_credit\"))\n",
    "# Count distinct \"CRediT\" elements for each \"doi\" and generate a new column \"paper_credit_num\"\n",
    "paper_credit_num = credit_explode_df.groupBy(\"year\", \"doi\").agg(countDistinct(\"credit_exploded\").alias(\"paper_credit_num\"))\n",
    "\n",
    "# Merge the results back into the main DataFrame\n",
    "description_df = description_df.join(paper_credit, [\"year\", \"doi\"], \"left_outer\").join(paper_credit_num, [\"year\", \"doi\"], \"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13f68a-c4d9-4ebd-be28-34c9ee95f04c",
   "metadata": {},
   "source": [
    "Figure 5. Average proportion of CRediT contribution types by discipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38432538-e9f9-4a6b-8e1e-a68e55a22eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, explode, avg\n",
    "\n",
    "# Select relevant columns from the description_df DataFrame\n",
    "figure3b_df = description_df.select(\"doi\", \"subjareas\", \"paper_credit\", \"row_count\", \"auid\", \"credit\")\n",
    "\n",
    "# Explode the \"credit\" column into separate rows\n",
    "credit_explode_df2 = figure3b_df.withColumn(\"credit_exploded\", explode(description_df[\"credit\"])) \n",
    "\n",
    "# Use collect_list to create a list of credits, naming it \"paper_credit_3b\"\n",
    "paper_credit_3b = credit_explode_df2.groupBy(\"doi\").agg(collect_list(\"credit_exploded\").alias(\"paper_credit_3b\"))\n",
    "\n",
    "# Join the new \"paper_credit_3b\" back to the original DataFrame\n",
    "figure3b_df = figure3b_df.join(paper_credit_3b, [\"doi\"], \"left_outer\")\n",
    "\n",
    "# Merge \"auid\" and \"CRediT\" columns\n",
    "figure3b_df = (\n",
    "    figure3b_df.groupBy(\"doi\", 'subjareas', 'row_count', 'paper_credit_3b')\n",
    "      .agg(collect_list(struct(\"auid\", \"credit\")).alias(\"Au\"))\n",
    ").drop(\"Au\")\n",
    "\n",
    "# Explode the \"paper_credit_3b\" array into separate rows\n",
    "exploded_df = figure3b_df.withColumn(\"paper_credit_3b\", explode(\"paper_credit_3b\"))\n",
    "\n",
    "# Count occurrences of different elements\n",
    "result_3b = exploded_df.groupBy(\"doi\", \"subjareas\", \"row_count\", \"paper_credit_3b\").count()\n",
    "# Calculate the percentage of each credit occurrence relative to the total row count\n",
    "result_3b = result_3b.withColumn(\"percentage\", result_3b[\"count\"] / result_3b[\"row_count\"])\n",
    "\n",
    "# Explode the \"subjareas\" column into separate rows\n",
    "exploded_df3 = result_3b.withColumn(\"subjareas\", explode(\"subjareas\"))\n",
    "\n",
    "# Calculate the average percentage of occurrences for each \"subjareas\" and \"paper_credit_3b\"\n",
    "data_3b = exploded_df3.groupBy(\"subjareas\", \"paper_credit_3b\").avg(\"percentage\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "data_3b.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8bc58-2d14-4f57-8050-d8204eb923f2",
   "metadata": {},
   "source": [
    "Figure 6. Average number of CRediT contribution types by author involved per article and field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7ed7a-4166-45a8-bb74-f461199e2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "number of contributions per article by fields\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import explode, count, collect_list, struct, avg\n",
    "\n",
    "# Calculate the average \"au_credit_num\" per group of [\"doi\"] (i.e., the average number of contributions per article)\n",
    "paper_au_mean_num = description_df.groupBy(\"doi\").agg(avg(\"au_credit_num\").alias(\"paper_au_credit_avg\"))\n",
    "# Join the average contributions back to the DataFrame\n",
    "figure3a_df = description_df.join(paper_au_mean_num, [\"doi\"], \"left_outer\")\n",
    "\n",
    "# Aggregate data into paper-level data\n",
    "paper_df = (\n",
    "    figure3a_df.groupBy(\"doi\", \"year\", 'subjareas', 'discipline', 'row_count', 'position_max', 'position_min', 'paper_credit', 'paper_credit_num', 'paper_au_credit_avg')\n",
    "      .agg(collect_list(struct(\"auid\", \"surname\", \"position\", \"credit\", \"au_credit_num\")).alias(\"Au\"))\n",
    ")\n",
    "\n",
    "# Calculate the count of each discipline\n",
    "# Use explode to expand the discipline array\n",
    "paper_df_exploded = paper_df.select(\"doi\", \"paper_au_credit_avg\", explode(\"discipline\").alias(\"discipline\"))\n",
    "discipline_counts_df = paper_df_exploded.groupBy(\"discipline\").agg(count(\"discipline\").alias(\"count\"))\n",
    "# Display the counts of each discipline without truncation\n",
    "discipline_counts_df.display(truncate=False)\n",
    "\n",
    "# Select different paper_au_credit_avg counts for each discipline\n",
    "average_df = paper_df_exploded.select(\"discipline\", \"paper_au_credit_avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f30388-a35b-4de5-8edc-8ab8db151db2",
   "metadata": {},
   "source": [
    "| Discipline         | Paper Count |\r\n",
    "|--------------------|-------------|\r\n",
    "| Health Sciences     | 73720       |\r\n",
    "| Life Sciences       | 164321      |\r\n",
    "| Physical Sciences   | 503141      |\r\n",
    "| Multidisciplinary   | 69542       |\r\n",
    "| Social Sciences     | 69915      |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d7401-ddc9-4b50-bbb5-e83d3f698d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define the conditions for categorizing the ranges\n",
    "conditions = [\n",
    "    (col(\"paper_au_credit_avg\") >= 1) & (col(\"paper_au_credit_avg\") < 2),\n",
    "    (col(\"paper_au_credit_avg\") >= 2) & (col(\"paper_au_credit_avg\") < 3),\n",
    "    (col(\"paper_au_credit_avg\") >= 3) & (col(\"paper_au_credit_avg\") < 4),\n",
    "    (col(\"paper_au_credit_avg\") >= 4) & (col(\"paper_au_credit_avg\") < 5),\n",
    "    (col(\"paper_au_credit_avg\") >= 5) & (col(\"paper_au_credit_avg\") < 6),\n",
    "    (col(\"paper_au_credit_avg\") >= 6) & (col(\"paper_au_credit_avg\") < 7),\n",
    "    (col(\"paper_au_credit_avg\") >= 7) & (col(\"paper_au_credit_avg\") < 8),\n",
    "    (col(\"paper_au_credit_avg\") >= 8)\n",
    "]\n",
    "\n",
    "# Define the labels for the ranges\n",
    "labels = [\"[1,2)\", \"[2,3)\", \"[3,4)\", \"[4,5)\", \"[5,6)\", \"[6,7)\", \"[7,8)\", \"[8,14]\"]\n",
    "\n",
    "# Add a new column to categorize \"paper_au_credit_avg\" into different ranges based on the conditions\n",
    "average_df = average_df.withColumn(\"range\", \n",
    "                                   when(conditions[0], labels[0])\n",
    "                                   .when(conditions[1], labels[1])\n",
    "                                   .when(conditions[2], labels[2])\n",
    "                                   .when(conditions[3], labels[3])\n",
    "                                   .when(conditions[4], labels[4])\n",
    "                                   .when(conditions[5], labels[5])\n",
    "                                   .when(conditions[6], labels[6])\n",
    "                                   .when(conditions[7], labels[7])\n",
    "                                   .otherwise(None))\n",
    "\n",
    "# Use groupBy and agg to count the occurrences of each range for different \"discipline\"\n",
    "result_3a = average_df.groupBy(\"discipline\", \"range\").count()\n",
    "\n",
    "# Calculate the percentage of counts for each discipline in relation to total counts\n",
    "result_3a = result_3a.withColumn(\"percentage\", \n",
    "                           when(result_3a[\"discipline\"] == \"Health Sciences\", result_3a[\"count\"]/73720)\n",
    "                           .otherwise(\n",
    "                               when(result_3a[\"discipline\"] == \"Life Sciences\", result_3a[\"count\"]/164321)\n",
    "                               .otherwise(\n",
    "                                   when(result_3a[\"discipline\"] == \"Physical Sciences\", result_3a[\"count\"]/503141)\n",
    "                                   .otherwise(\n",
    "                                       when(result_3a[\"discipline\"] == \"Multidisciplinary\", result_3a[\"count\"]/69542)\n",
    "                                       .otherwise(\n",
    "                                           when(result_3a[\"discipline\"] == \"Social Sciences\", result_3a[\"count\"]/69915)\n",
    "                                           .otherwise(0)\n",
    "                                       )\n",
    "                                   )\n",
    "                               )\n",
    "                           )\n",
    "                          )\n",
    "\n",
    "# Display the results\n",
    "result_3a.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313080be-74ee-47c9-a484-f51d83cb7f23",
   "metadata": {},
   "source": [
    "Figure 7. Co-occurrence of CRediT contribution types by field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9aa93-2877-4b67-8c0e-346aa7c59a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All fields data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56c6bf-8bca-4ffe-b362-bc40c5d6c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Select relevant columns from the description_df DataFrame\n",
    "df_figure4 = description_df.select(\"discipline\", \"credit\")\n",
    "\n",
    "# Define the list of all possible elements\n",
    "elements = [\"Conceptualization\", \"Data curation\", \"Formal analysis\", \"Funding acquisition\", \n",
    "            \"Investigation\", \"Methodology\", \"Project administration\", \"Resources\", \n",
    "            \"Software\", \"Supervision\", \"Validation\", \"Visualization\", \n",
    "            \"Writing – original draft\", \"Writing – review & editing\"]\n",
    "\n",
    "# Generate all combinations of the elements taken two at a time\n",
    "element_combinations = list(combinations(elements, 2))\n",
    "\n",
    "# Create an empty result list\n",
    "figure4_result_df = []\n",
    "\n",
    "# Iterate through each combination to calculate counts\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count the number of records containing either of the two elements\n",
    "    count_either = df_figure4.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count the number of records containing both elements\n",
    "    count_both = df_figure4.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_result_df.append((element1, element2, count_either, count_both))\n",
    "\n",
    "# Create a result DataFrame from the result list\n",
    "figure4_result_df = spark.createDataFrame(figure4_result_df, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences for both elements\n",
    "figure4_result_df = figure4_result_df.withColumn(\"percentage\", figure4_result_df[\"count of both\"] / figure4_result_df[\"count of either\"])\n",
    "# Display the resulting DataFrame\n",
    "figure4_result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706ea26-e68e-47e7-aaa7-e60623e439d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each decipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1b3e6-c11d-4fe2-b878-d7f3fab1bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from itertools import combinations\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "# Explode the discipline and credit fields from the description_df DataFrame\n",
    "figure4_explode = description_df.select(explode(\"discipline\").alias(\"discipline\"), \"credit\")\n",
    "\n",
    "# Define a list of all possible CRediT elements\n",
    "elements = [\"Conceptualization\", \"Data curation\", \"Formal analysis\", \"Funding acquisition\", \n",
    "            \"Investigation\", \"Methodology\", \"Project administration\", \"Resources\", \n",
    "            \"Software\", \"Supervision\", \"Validation\", \"Visualization\", \n",
    "            \"Writing – original draft\", \"Writing – review & editing\"]\n",
    "\n",
    "# Generate all possible pairs of CRediT elements using combinations\n",
    "element_combinations = list(combinations(elements, 2))\n",
    "\n",
    "\n",
    "# Analyze Health Sciences data\n",
    "print(\"Health Sciences data:\")\n",
    "# Filter data for the Health Sciences discipline\n",
    "health_data = figure4_explode.filter(figure4_explode[\"discipline\"] == \"Health Sciences\")\n",
    "# Create an empty list to store results for Health Sciences\n",
    "figure4_health = []\n",
    "# Iterate over each combination of elements\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count how many times either of the two elements appears in the data\n",
    "    count_either = health_data.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count how many times both elements appear simultaneously in the data\n",
    "    count_both = health_data.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_health.append((element1, element2, count_either, count_both))\n",
    "# Create a DataFrame from the results for Health Sciences\n",
    "figure4_health = spark.createDataFrame(figure4_health, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences where both elements are present\n",
    "figure4_health = figure4_health.withColumn(\"percentage\", figure4_health[\"count of both\"] / figure4_health[\"count of either\"])\n",
    "# Display the results for Health Sciences\n",
    "figure4_health.display(truncate=False)\n",
    "\n",
    "\n",
    "# Analyze Life Sciences data\n",
    "print(\"Life Sciences data:\")\n",
    "# Filter data for the Life Sciences discipline\n",
    "life_data = figure4_explode.filter(figure4_explode[\"discipline\"] == \"Life Sciences\")\n",
    "# Create an empty list to store results for Life Sciences\n",
    "figure4_life = []\n",
    "# Iterate over each combination of elements\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count how many times either of the two elements appears in the data\n",
    "    count_either = life_data.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count how many times both elements appear simultaneously in the data\n",
    "    count_both = life_data.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_life.append((element1, element2, count_either, count_both))\n",
    "# Create a DataFrame from the results for Life Sciences\n",
    "figure4_life = spark.createDataFrame(figure4_life, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences where both elements are present\n",
    "figure4_life = figure4_life.withColumn(\"percentage\", figure4_life[\"count of both\"] / figure4_life[\"count of either\"])\n",
    "# Display the results for Life Sciences\n",
    "figure4_life.display(truncate=False)\n",
    "\n",
    "\n",
    "# Analyze Multidisciplinary data\n",
    "print(\"Multidisciplinary data:\")\n",
    "# Filter data for the Multidisciplinary discipline\n",
    "multi_data = figure4_explode.filter(figure4_explode[\"discipline\"] == \"Multidisciplinary\")\n",
    "# Create an empty list to store results for Multidisciplinary\n",
    "figure4_multi = []\n",
    "# Iterate over each combination of elements\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count how many times either of the two elements appears in the data\n",
    "    count_either = multi_data.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count how many times both elements appear simultaneously in the data\n",
    "    count_both = multi_data.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_multi.append((element1, element2, count_either, count_both))\n",
    "# Create a DataFrame from the results for Multidisciplinary\n",
    "figure4_multi = spark.createDataFrame(figure4_multi, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences where both elements are present\n",
    "figure4_multi = figure4_multi.withColumn(\"percentage\", figure4_multi[\"count of both\"] / figure4_multi[\"count of either\"])\n",
    "# Display the results for Multidisciplinary\n",
    "figure4_multi.display(truncate=False)\n",
    "\n",
    "\n",
    "# Analyze Physical Sciences data\n",
    "print(\"Physical Sciences data:\")\n",
    "# Filter data for the Physical Sciences discipline\n",
    "physical_data = figure4_explode.filter(figure4_explode[\"discipline\"] == \"Physical Sciences\")\n",
    "# Create an empty list to store results for Physical Sciences\n",
    "figure4_physical = []\n",
    "# Iterate over each combination of elements\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count how many times either of the two elements appears in the data\n",
    "    count_either = physical_data.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count how many times both elements appear simultaneously in the data\n",
    "    count_both = physical_data.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_physical.append((element1, element2, count_either, count_both))\n",
    "# Create a DataFrame from the results for Physical Sciences\n",
    "figure4_physical = spark.createDataFrame(figure4_physical, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences where both elements are present\n",
    "figure4_physical = figure4_physical.withColumn(\"percentage\", figure4_physical[\"count of both\"] / figure4_physical[\"count of either\"])\n",
    "# Display the results for Physical Sciences\n",
    "figure4_physical.display(truncate=False)\n",
    "\n",
    "\n",
    "# Analyze Social Sciences data\n",
    "print(\"Social Sciences data:\")\n",
    "# Filter data for the Social Sciences discipline\n",
    "social_data = figure4_explode.filter(figure4_explode[\"discipline\"] == \"Social Sciences\")\n",
    "# Create an empty list to store results for Social Sciences\n",
    "figure4_social = []\n",
    "# Iterate over each combination of elements\n",
    "for combination in element_combinations:\n",
    "    element1, element2 = combination\n",
    "    # Count how many times either of the two elements appears in the data\n",
    "    count_either = social_data.filter((array_contains(\"credit\", element1)) | (array_contains(\"credit\", element2))).count()\n",
    "    # Count how many times both elements appear simultaneously in the data\n",
    "    count_both = social_data.filter(array_contains(\"credit\", element1) & array_contains(\"credit\", element2)).count()\n",
    "    # Append the results to the list\n",
    "    figure4_social.append((element1, element2, count_either, count_both))\n",
    "# Create a DataFrame from the results for Social Sciences\n",
    "figure4_social = spark.createDataFrame(figure4_social, schema=[\"element1\", \"element2\", \"count of either\", \"count of both\"])\n",
    "# Calculate the percentage of occurrences where both elements are present\n",
    "figure4_social = figure4_social.withColumn(\"percentage\", figure4_social[\"count of both\"] / figure4_social[\"count of either\"])\n",
    "# Display the results for Social Sciences\n",
    "figure4_social.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c545abe-c23d-48ac-a73e-ad1371bb8656",
   "metadata": {},
   "source": [
    "Figure 8. Relation between CRediT contributions and author positions by field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e689d8-953e-4ed3-9dbb-fdc77d1516ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate the number of authors for each position. Includes full data and individual disciplines.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import max, when, explode\n",
    "\n",
    "# Select relevant columns from description_df to create figure5_df\n",
    "figure5_df = description_df.select(\"doi\", \"auid\", \"position\", \"credit\", \"row_count\", \"position_max\", \"position_min\", \"discipline\")\n",
    "\n",
    "# Create a new column \"position2\" to categorize authorship based on their position\n",
    "figure5_df = figure5_df.withColumn(\"position2\",\n",
    "                                      when(figure5_df[\"position\"] == figure5_df[\"position_min\"], \"First author\")  # Label as \"First author\" if position is minimum\n",
    "                                      .when(figure5_df[\"position\"] == figure5_df[\"position_max\"], \"Last author\")   # Label as \"Last author\" if position is maximum\n",
    "                                      .otherwise(\"Middle author\"))  # Label as \"Middle author\" for all other positions\n",
    "\n",
    "# Display the modified DataFrame\n",
    "figure5_df.display()\n",
    "\n",
    "# Count the number of authors for each position category in the total dataset\n",
    "print(\"total data:\")\n",
    "figure5_df.groupBy(\"position2\").count().display()\n",
    "\n",
    "# Explode the discipline column and create a new DataFrame for further analysis\n",
    "explode_figure5 = figure5_df.select(explode(\"discipline\").alias(\"discipline\"), \"position2\", \"credit\")\n",
    "\n",
    "# Count the number of authors in each position category for Health Sciences\n",
    "print(\"Health Sciences data:\")\n",
    "explode_figure5.filter(explode_figure5[\"discipline\"] == \"Health Sciences\").groupBy(\"position2\").count().display()\n",
    "\n",
    "# Count the number of authors in each position category for Life Sciences\n",
    "print(\"Life Sciences data:\")\n",
    "explode_figure5.filter(explode_figure5[\"discipline\"] == \"Life Sciences\").groupBy(\"position2\").count().display()\n",
    "\n",
    "# Count the number of authors in each position category for Multidisciplinary\n",
    "print(\"Multidisciplinary data:\")\n",
    "explode_figure5.filter(explode_figure5[\"discipline\"] == \"Multidisciplinary\").groupBy(\"position2\").count().display()\n",
    "\n",
    "# Count the number of authors in each position category for Physical Sciences\n",
    "print(\"Physical Sciences data:\")\n",
    "explode_figure5.filter(explode_figure5[\"discipline\"] == \"Physical Sciences\").groupBy(\"position2\").count().display()\n",
    "\n",
    "# Count the number of authors in each position category for Social Sciences\n",
    "print(\"Social Sciences data:\")\n",
    "explode_figure5.filter(explode_figure5[\"discipline\"] == \"Social Sciences\").groupBy(\"position2\").count().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6cc20-2d2c-471c-a291-312ebd489dc8",
   "metadata": {},
   "source": [
    "# Total Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 2,590,052    |\r\n",
    "| First author   | 714,671      |\r\n",
    "| Last author    | 701,041      |\r\n",
    "\r\n",
    "# Health Sciences Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 364,654      |\r\n",
    "| First author   | 73,723       |\r\n",
    "| Last author    | 72,821       |\r\n",
    "\r\n",
    "# Life Sciences Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 713,629      |\r\n",
    "| First author   | 164,330      |\r\n",
    "| Last author    | 162,339      |\r\n",
    "\r\n",
    "# Multidisciplinary Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 305,681      |\r\n",
    "| First author   | 69,564       |\r\n",
    "| Last author    | 68,416       |\r\n",
    "\r\n",
    "# Physical Sciences Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 1,654,716    |\r\n",
    "| First author   | 503,129      |\r\n",
    "| Last author    | 493,423      |\r\n",
    "\r\n",
    "# Social Sciences Data\r\n",
    "\r\n",
    "| Position       | Author Count |\r\n",
    "|----------------|--------------|\r\n",
    "| Middle author  | 160,627      |\r\n",
    "| First author   | 69,912       |\r\n",
    "| Last author    | 67,060       |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4c512-69b6-4fef-9031-e895558393f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, when\n",
    "\n",
    "# Explode the 'credit' column to create a new DataFrame with individual credits for each author position\n",
    "explode_position = figure5_df.select(\"position2\", explode(\"credit\").alias(\"CRediT\"))\n",
    "\n",
    "# Group by author position and credit, counting the occurrences of each combination\n",
    "position_credit = explode_position.groupBy(\"position2\", \"CRediT\").count()\n",
    "\n",
    "# Calculate the percentage of authors for each position based on predefined total counts\n",
    "position_credit = position_credit.withColumn(\"percentage\",\n",
    "                                      when(position_credit[\"position2\"] == \"First author\", position_credit[\"count\"] / 714671)  # Total for First authors\n",
    "                                      .when(position_credit[\"position2\"] == \"Middle author\", position_credit[\"count\"] / 2590052)  # Total for Middle authors\n",
    "                                      .when(position_credit[\"position2\"] == \"Last author\", position_credit[\"count\"] / 701041))  # Total for Last authors\n",
    "\n",
    "# Rename 'position2' to 'Author Position' for clarity\n",
    "position_credit = position_credit.withColumnRenamed(\"position2\", \"Author Position\")\n",
    "\n",
    "# Display the total data with counts and percentages\n",
    "print(\"total data:\")\n",
    "position_credit.display(truncate=False)\n",
    "\n",
    "# Rename the exploded DataFrame column for consistency\n",
    "explode_figure5 = explode_figure5.withColumnRenamed(\"position2\", \"Author Position\")\n",
    "\n",
    "# Analyze Health Sciences discipline data\n",
    "print(\"Health Sciences data:\")\n",
    "health_explode = explode_figure5.filter(explode_figure5[\"discipline\"] == \"Health Sciences\").select(\"Author Position\", explode(\"credit\").alias(\"CRediT\")).groupBy(\"Author Position\", \"CRediT\").count()\n",
    "health_explode.withColumn(\"percentage\",\n",
    "        when(health_explode[\"Author Position\"] == \"First author\", health_explode[\"count\"] / 73723)  # Total for First authors in Health Sciences\n",
    "        .when(health_explode[\"Author Position\"] == \"Middle author\", health_explode[\"count\"] / 364654)  # Total for Middle authors\n",
    "        .when(health_explode[\"Author Position\"] == \"Last author\", health_explode[\"count\"] / 72821)).display()  # Total for Last authors\n",
    "\n",
    "# Analyze Life Sciences discipline data\n",
    "print(\"Life Sciences data:\")\n",
    "life_explode = explode_figure5.filter(explode_figure5[\"discipline\"] == \"Life Sciences\").select(\"Author Position\", explode(\"credit\").alias(\"CRediT\")).groupBy(\"Author Position\", \"CRediT\").count()\n",
    "life_explode.withColumn(\"percentage\",\n",
    "        when(life_explode[\"Author Position\"] == \"First author\", life_explode[\"count\"] / 164330)  # Total for First authors in Life Sciences\n",
    "        .when(life_explode[\"Author Position\"] == \"Middle author\", life_explode[\"count\"] / 713629)  # Total for Middle authors\n",
    "        .when(life_explode[\"Author Position\"] == \"Last author\", life_explode[\"count\"] / 162339)).display()  # Total for Last authors\n",
    "\n",
    "# Analyze Multidisciplinary discipline data\n",
    "print(\"Multidisciplinary data:\")\n",
    "Multidisciplinary_explode = explode_figure5.filter(explode_figure5[\"discipline\"] == \"Multidisciplinary\").select(\"Author Position\", explode(\"credit\").alias(\"CRediT\")).groupBy(\"Author Position\", \"CRediT\").count()\n",
    "Multidisciplinary_explode.withColumn(\"percentage\",\n",
    "        when(Multidisciplinary_explode[\"Author Position\"] == \"First author\", Multidisciplinary_explode[\"count\"] / 69564)  # Total for First authors in Multidisciplinary\n",
    "        .when(Multidisciplinary_explode[\"Author Position\"] == \"Middle author\", Multidisciplinary_explode[\"count\"] / 305681)  # Total for Middle authors\n",
    "        .when(Multidisciplinary_explode[\"Author Position\"] == \"Last author\", Multidisciplinary_explode[\"count\"] / 68416)).display()  # Total for Last authors\n",
    "\n",
    "# Analyze Physical Sciences discipline data\n",
    "print(\"Physical Sciences data:\")\n",
    "Physical_explode = explode_figure5.filter(explode_figure5[\"discipline\"] == \"Physical Sciences\").select(\"Author Position\", explode(\"credit\").alias(\"CRediT\")).groupBy(\"Author Position\", \"CRediT\").count()\n",
    "Physical_explode.withColumn(\"percentage\",\n",
    "        when(Physical_explode[\"Author Position\"] == \"First author\", Physical_explode[\"count\"] / 503129)  # Total for First authors in Physical Sciences\n",
    "        .when(Physical_explode[\"Author Position\"] == \"Middle author\", Physical_explode[\"count\"] / 1654716)  # Total for Middle authors\n",
    "        .when(Physical_explode[\"Author Position\"] == \"Last author\", Physical_explode[\"count\"] / 493423)).display()  # Total for Last authors\n",
    "\n",
    "# Analyze Social Sciences discipline data\n",
    "print(\"Social Sciences data:\")\n",
    "Social_explode = explode_figure5.filter(explode_figure5[\"discipline\"] == \"Social Sciences\").select(\"Author Position\", explode(\"credit\").alias(\"CRediT\")).groupBy(\"Author Position\", \"CRediT\").count()\n",
    "Social_explode.withColumn(\"percentage\",\n",
    "        when(Social_explode[\"Author Position\"] == \"First author\", Social_explode[\"count\"] / 69912)  # Total for First authors in Social Sciences\n",
    "        .when(Social_explode[\"Author Position\"] == \"Middle author\", Social_explode[\"count\"] / 160627)  # Total for Middle authors\n",
    "        .when(Social_explode[\"Author Position\"] == \"Last author\", Social_explode[\"count\"] / 67060)).display()  # Total for Last authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b7f4b-c635-49a3-a908-7be7f7324562",
   "metadata": {},
   "source": [
    "The chi-square test of independence between author position and CRediT categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876491-8fce-4034-ac1b-58e3f075ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square-data\n",
    "\n",
    "from pyspark.sql.functions import col, explode, array_contains, udf, when, max\n",
    "from pyspark.sql.types import IntegerType\n",
    "import ast\n",
    "\n",
    "figure5_df = description_df.select(\"doi\", \"auid\", \"position\", \"credit\", \"paper_credit\", \"row_count\", \"position_max\", \"position_min\", \"discipline\")\n",
    "figure5_df = figure5_df.withColumn(\"position2\",\n",
    "                                      when(figure5_df[\"position\"] == figure5_df[\"position_min\"], \"First author\")\n",
    "                                      .when(figure5_df[\"position\"] == figure5_df[\"position_max\"], \"Last author\")\n",
    "                                      .otherwise(\"Middle author\"))\n",
    "\n",
    "figure5_df.filter(figure5_df[\"doi\"] == \"10.1016/j.aca.2019.05.051\").display()\n",
    "\n",
    "# 将字符串形式的列表转换为真正的 Python 列表\n",
    "str_to_list_udf = udf(lambda x: ast.literal_eval(x) if isinstance(x, str) else x, 'array<string>')\n",
    "df = figure5_df.withColumn(\"credit_list\", str_to_list_udf(col(\"credit\"))) \\\n",
    "       .withColumn(\"paper_credit_list\", str_to_list_udf(col(\"paper_credit\")))\n",
    "\n",
    "# 展开 paper_credit_list\n",
    "df_exploded = df.withColumn(\"paper_credit_item\", explode(col(\"paper_credit_list\")))\n",
    "\n",
    "# 判断 credit_list 是否包含当前 paper_credit_item\n",
    "chi_final = df_exploded.withColumn(\"Y/N\", \n",
    "                                  array_contains(col(\"credit_list\"), col(\"paper_credit_item\")).cast(IntegerType()))\n",
    "\n",
    "# 选择最终列\n",
    "chi_final = chi_final.select(\"position2\", \"paper_credit_item\", \"Y/N\", \"doi\", \"auid\", \"discipline\")\n",
    "chi_final.filter(chi_final[\"doi\"] == \"10.1016/j.aca.2019.05.051\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c8b12-32e4-4a0b-93b8-ec1dd94df8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square-total credit\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = chi_final.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Step 5: 输出结果\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")  #保留三位小数\n",
    "print(\"Expected frequencies:\\n\", expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25876c86-ea13-4c2e-8116-da9142ee01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square-per_credit\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 设置 DataFrame 输出时浮点数保留三位小数\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = chi_final.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370683d8-6907-4cbd-bd27-660298aaefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-fields\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi_explode = chi_final.select(explode(\"discipline\").alias(\"discipline\"), \"position2\", \"paper_credit_item\", \"Y/N\", \"doi\", \"auid\")\n",
    "\n",
    "# 设置 DataFrame 输出时浮点数保留三位小数\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "print(\"Health Sciences data:\")\n",
    "health_chi_data = chi_explode.filter(chi_explode[\"discipline\"] == \"Health Sciences\")\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = health_chi_data.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "# Step 5: 输出结果\n",
    "print(\"position_credit:\")\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"per credit:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Life Sciences data:\")\n",
    "life_chi_data = chi_explode.filter(chi_explode[\"discipline\"] == \"Life Sciences\")\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = life_chi_data.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "# Step 5: 输出结果\n",
    "print(\"position_credit:\")\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"per credit:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Multidisciplinary data:\")\n",
    "multi_chi_data = chi_explode.filter(chi_explode[\"discipline\"] == \"Multidisciplinary\")\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = multi_chi_data.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "# Step 5: 输出结果\n",
    "print(\"position_credit:\")\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"per credit:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Physical Sciences data:\")\n",
    "physical_chi_data = chi_explode.filter(chi_explode[\"discipline\"] == \"Physical Sciences\")\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = physical_chi_data.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "# Step 5: 输出结果\n",
    "print(\"position_credit:\")\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"per credit:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Social Sciences data:\")\n",
    "social_chi_data = chi_explode.filter(chi_explode[\"discipline\"] == \"Social Sciences\")\n",
    "\n",
    "# Step 1: 转为 Pandas DataFrame\n",
    "df_pd = social_chi_data.select(\"position2\", \"paper_credit_item\", \"Y/N\").toPandas()\n",
    "# Step 2: 只保留 Y/N = 1 的行（实际参与）\n",
    "df_pd_participated = df_pd[df_pd['Y/N'] == 1]\n",
    "# Step 3: 构建列联表\n",
    "contingency_table = pd.crosstab(df_pd_participated['position2'], df_pd_participated['paper_credit_item'])\n",
    "# Step 4: 进行卡方检验\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "# Step 5: 输出结果\n",
    "print(\"position_credit:\")\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(f\"p-value: {p:.3f}\")\n",
    "\n",
    "# Step 2: 获取所有贡献类型列表\n",
    "contribution_types = df_pd['paper_credit_item'].unique()\n",
    "# Step 3: 循环每个贡献类型做卡方检验\n",
    "results = []\n",
    "for contrib in contribution_types:\n",
    "    # 取当前贡献类型的所有行\n",
    "    df_subset = df_pd[df_pd['paper_credit_item'] == contrib]\n",
    "    \n",
    "    # 构建完整的 2×3 列联表 (rows=Y/N, columns=position2)\n",
    "    contingency_table = pd.crosstab(df_subset['Y/N'], df_subset['position2'])\n",
    "    \n",
    "    # 如果行或列不足2，chi2_contingency 会报错，补全0\n",
    "    if contingency_table.shape[0] < 2:\n",
    "        contingency_table.loc[0] = contingency_table.loc.get(0, 0)\n",
    "        contingency_table.loc[1] = contingency_table.loc.get(1, 0)\n",
    "    if contingency_table.shape[1] < 3:\n",
    "        for col in ['First author','Middle author','Last author']:\n",
    "            if col not in contingency_table.columns:\n",
    "                contingency_table[col] = 0\n",
    "        # 保证列顺序\n",
    "        contingency_table = contingency_table[['First author','Middle author','Last author']]\n",
    "    \n",
    "    # 卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results.append({'paper_credit_item': contrib, 'chi2': chi2, 'p_value': p, 'dof': dof})\n",
    "\n",
    "# Step 4: 转为 DataFrame 输出\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"per credit:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
